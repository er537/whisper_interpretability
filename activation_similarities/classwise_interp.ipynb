{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e09721d-545a-4440-b622-aabd0ce4bec0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ellenar/git/acoustic_activations_interp\n"
     ]
    }
   ],
   "source": [
    "cd ~/git/acoustic_activations_interp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caa60800-2435-4e18-b58e-9cd4ba59ff76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "723c3ea3-27ff-4e3d-af2e-c4642e66879f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.16) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "/venv/lib/python3.8/site-packages/whisper/timing.py:58: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def backtrace(trace: np.ndarray):\n"
     ]
    }
   ],
   "source": [
    "from activation_module import WhipserActivationModule\n",
    "from dataset import ClasswiseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "425fb204-7868-4693-8d1d-938dfe7e6bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = 'encoder.blocks.0'\n",
    "samples_per_class = 480_000\n",
    "batch_size = int(samples_per_class / 480_000)\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d34aa92-9743-4ac6-be47-1a885c225006",
   "metadata": {},
   "source": [
    "### NON-SPEECH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3825e09c-b999-4f95-bb61-fffb673707f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label:NON_SPEECH, Number of Samples:480000\n",
      "Decoding:torch.Size([1, 80, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ellenar/git/acoustic_activations_interp/dataset.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mels = torch.tensor(whisper.log_mel_spectrogram(audio)).to(device)\n"
     ]
    }
   ],
   "source": [
    "module = WhipserActivationModule(activations_to_cache=[layer], data_class=\"NON_SPEECH\", samples_per_class=samples_per_class, batch_size=batch_size)\n",
    "module.forward()\n",
    "non_speech_activations = module.activations[f'{layer}.output']\n",
    "non_speech_activations = torch.reshape(non_speech_activations, (-1, 384))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9873c4-4736-46f2-a910-1a7391860036",
   "metadata": {},
   "source": [
    "### SPEECH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "319b438e-97d7-44ea-8704-5d732e53020d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label:SPEECH, Number of Samples:480000\n",
      "Decoding:torch.Size([1, 80, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ellenar/git/acoustic_activations_interp/dataset.py:94: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mels = torch.tensor(whisper.log_mel_spectrogram(audio)).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1500, 1536])\n"
     ]
    }
   ],
   "source": [
    "module = WhipserActivationModule(activations_to_cache=[layer], data_class=\"SPEECH\", samples_per_class=samples_per_class, batch_size=batch_size)\n",
    "module.forward()\n",
    "speech_activations = module.activations[f'{layer}.output']\n",
    "print(speech_activations.shape)\n",
    "speech_activations = torch.reshape(speech_activations, (-1, 384))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6065db9e-b0fd-452a-b2ef-7a724b6dc2c5",
   "metadata": {},
   "source": [
    "### Compute dataset mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11177ade-21b3-40ca-ade4-d1d25c14e551",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_activations = torch.cat((speech_activations, non_speech_activations), dim=0)\n",
    "mean_activation = torch.mean(all_activations, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b36d98-a23e-4fec-a354-f252dd7b16f4",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d455c91-e480-4b3f-b97b-fac543c21d3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "speech_normed = (speech_activations / speech_activations.norm(dim=1)[:, None])\n",
    "non_speech_normed = (non_speech_activations / non_speech_activations.norm(dim=1)[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70ce07b4-a727-4b49-a8be-e338f99352a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# speech_normed = speech_activations - mean_activation\n",
    "# speech_normed = (speech_normed / speech_normed.norm(dim=1)[:, None])\n",
    "# non_speech_normed = non_speech_activations - mean_activation\n",
    "# non_speech_normed = (non_speech_normed / non_speech_normed.norm(dim=1)[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030f1ab2-be01-4dda-93bf-4b1f45c48c96",
   "metadata": {},
   "source": [
    "### Get cosine sim scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "328d6481-86f6-4f7f-a5cc-e828016e3b4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non speech tensor(0.1542)\n",
      "Speech tensor(0.1562)\n",
      "Inter tensor(0.1541)\n"
     ]
    }
   ],
   "source": [
    "non_speech_sim = torch.mm(non_speech_normed, non_speech_normed.transpose(0,1))\n",
    "print('Non speech', torch.mean(non_speech_sim))\n",
    "speech_sim = torch.mm(speech_normed, speech_normed.transpose(0,1))\n",
    "torch.mean(speech_sim)\n",
    "print('Speech', torch.mean(speech_sim))\n",
    "inter_class_sim = torch.mm(speech_normed, non_speech_normed.transpose(0,1))\n",
    "print('Inter', torch.mean(inter_class_sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e8e94f-bf1b-497e-b827-d0d97f9a0355",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get euclidean scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64e960a7-adf7-4837-9b6a-bc1e28ce40c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_speech_dist = torch.cdist(non_speech_activations, non_speech_activations, p=2.0)\n",
    "speech_dist = torch.cdist(speech_activations, speech_activations, p=2.0)\n",
    "inter_dist = torch.cdist(speech_activations, non_speech_activations, p=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bdd80e-0ce7-49f4-8b02-26ddfcb78669",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(torch.mean(speech_dist), torch.mean(non_speech_dist), torch.mean(inter_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ccf1c5-692d-4827-abc0-6eb0f6582c6d",
   "metadata": {},
   "source": [
    "### Compute SVDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14f9dee-7777-4730-bb66-35352ece09ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: check SVD is along correct dim\n",
    "import numpy as np\n",
    "U, S, Vh = np.linalg.svd(speech_normed, full_matrices=False, compute_uv=True, hermitian=False)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f63626-b668-4da7-b44b-50656e84afca",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vh = np.linalg.svd(non_speech_normed, full_matrices=False, compute_uv=True, hermitian=False)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe1aeae-cabf-4e96-bb99-3f092e0a2696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "U, S, Vh = np.linalg.svd(torch.cat((non_speech_normed, speech_normed), dim=0), full_matrices=False, compute_uv=True, hermitian=False)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a892b3-1d6e-4077-8d9e-cbbf85bbd8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
