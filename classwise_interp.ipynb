{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e09721d-545a-4440-b622-aabd0ce4bec0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ellenar/git/acoustic_activations_interp\n"
     ]
    }
   ],
   "source": [
    "cd ~/git/acoustic_activations_interp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caa60800-2435-4e18-b58e-9cd4ba59ff76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb82ae6-fda4-4210-84b5-6e0c222f850a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "723c3ea3-27ff-4e3d-af2e-c4642e66879f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.16) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "/venv/lib/python3.8/site-packages/whisper/timing.py:58: NumbaDeprecationWarning: \u001b[1mThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\u001b[0m\n",
      "  def backtrace(trace: np.ndarray):\n"
     ]
    }
   ],
   "source": [
    "from activation_module import WhipserActivationModule\n",
    "from dataset import ClasswiseDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "508f2626-c8d5-4227-8fc2-440c122e8be1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label:NON_SPEECH, Number of Samples:3000\n"
     ]
    }
   ],
   "source": [
    "module = WhipserActivationModule(activations_to_cache=['encoder.blocks.0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c59b3e3b-1f87-4ea9-9b04-ff9b337df478",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding:torch.Size([10, 80, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ellenar/git/acoustic_activations_interp/dataset.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mels = torch.tensor(whisper.log_mel_spectrogram(audio)).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding:torch.Size([10, 80, 3000])\n",
      "Decoding:torch.Size([10, 80, 3000])\n",
      "Decoding:torch.Size([10, 80, 3000])\n",
      "Decoding:torch.Size([10, 80, 3000])\n",
      "Decoding:torch.Size([10, 80, 3000])\n",
      "Decoding:torch.Size([10, 80, 3000])\n",
      "Decoding:torch.Size([10, 80, 3000])\n",
      "Decoding:torch.Size([10, 80, 3000])\n",
      "Decoding:torch.Size([10, 80, 3000])\n"
     ]
    }
   ],
   "source": [
    "module.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3825e09c-b999-4f95-bb61-fffb673707f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1500, 384])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_speech_activations = module.activations['encoder.blocks.0.output']\n",
    "non_speech_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be22081a-228e-4d79-b815-11b6efebb1dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150000, 384])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_speech_activations = torch.reshape(non_speech_activations, (-1, 384))\n",
    "non_speech_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f65e5e5-3494-4ad5-b418-48f0193e6529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_speech_activations = (non_speech_activations / non_speech_activations.norm(dim=1)[:, None])\n",
    "res = torch.mm(non_speech_activations, non_speech_activations.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cc2734e-2f35-4b4f-bcb9-7f56946aed41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([150000, 150000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a26e58a-bf0d-46e4-bece-35cb4dd69753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8391)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "319b438e-97d7-44ea-8704-5d732e53020d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class label:SPEECH, Number of Samples:3000\n",
      "Decoding:torch.Size([10, 80, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ellenar/git/acoustic_activations_interp/dataset.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mels = torch.tensor(whisper.log_mel_spectrogram(audio)).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding:torch.Size([10, 80, 3000])\n",
      "Decoding:torch.Size([10, 80, 3000])\n",
      "Decoding:torch.Size([10, 80, 3000])\n",
      "Decoding:torch.Size([10, 80, 3000])\n",
      "Decoding:torch.Size([10, 80, 3000])\n",
      "Decoding:torch.Size([10, 80, 3000])\n",
      "Decoding:torch.Size([10, 80, 3000])\n",
      "Decoding:torch.Size([10, 80, 3000])\n",
      "Decoding:torch.Size([10, 80, 3000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([150000, 384])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = WhipserActivationModule(activations_to_cache=['encoder.blocks.0'], data_class=\"SPEECH\")\n",
    "module.forward()\n",
    "speech_activations = module.activations['encoder.blocks.0.output']\n",
    "speech_activations = torch.reshape(non_speech_activations, (-1, 384))\n",
    "speech_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c26fee8-3eb7-4158-9363-222a6457d057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3250bbf-9160-4695-8ddc-2591c1385c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   9601.3662,    6521.7856,  -45004.2070,  ...,    9828.0488,\n",
       "           31612.8691,   12948.3662],\n",
       "        [   6521.7856,   11033.4531,  -16856.3008,  ...,    5924.6875,\n",
       "           20545.5625,    8993.9805],\n",
       "        [ -45004.2070,  -16856.3008,  296998.1875,  ...,  -63932.6719,\n",
       "         -177984.5156,  -71875.8516],\n",
       "        ...,\n",
       "        [   9828.0488,    5924.6875,  -63932.6719,  ...,  148269.4844,\n",
       "           41361.3594,   17099.8145],\n",
       "        [  31612.8691,   20545.5625, -177984.5156,  ...,   41361.3594,\n",
       "          118980.5000,   48819.9219],\n",
       "        [  12948.3662,    8993.9805,  -71875.8516,  ...,   17099.8145,\n",
       "           48819.9219,   20690.3730]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99aaee2-0b17-47d2-ac97-1eb2ccaa5e71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
